# Docker









# Ollama

Ollama是一个开源的本地大语言模型（LLM）运行框架，由ollama/ollama开发，可在Windows、macOS和Linux系统上运行，能让用户在本地轻松部署和运行大语言模型，而无需依赖云端服务，具体信息如下：

## 主要特点

- **安装便捷**：提供简单的安装脚本，适用于 Linux、macOS 和 Windows 系统。以 Linux 系统为例，在终端执行一行命令，就能完成安装，极大降低了技术门槛，方便普通用户上手。
- **模型丰富**：支持运行多种流行的开源大语言模型，如 Llama 2、Mistral、Falcon 等，用户可以根据自己的需求和硬件性能选择合适的模型，还在不断添加对新模型的支持。
- **使用简单**：通过命令行或者 HTTP API 就能与模型进行交互。在命令行中输入简短指令，就能让模型生成文本、回答问题等，对于开发者来说，也可以通过 HTTP API 轻松将 Ollama 集成到自己的应用程序中。
- **资源可控**：在本地运行模型，数据不会被发送到云端，能有效保护用户隐私。并且可以根据硬件资源，合理配置模型运行参数，避免因网络波动影响使用体验。

## 应用场景

- **个人学习**：学生和研究人员可以利用它在本地探索大语言模型的能力，进行自然语言处理相关的学习和实验，例如训练自定义模型、研究模型性能等。
- **小型项目开发**：开发者在开发需要使用大语言模型的小型应用时，使用 Ollama 可以快速搭建原型，降低开发成本和对云端服务的依赖，像开发本地智能客服、文本生成工具等。
- **隐私敏感场景**：对于一些对数据隐私要求极高的场景，如企业内部文档处理、医疗数据交互等，Ollama 的本地运行特性可以确保数据的安全和隐私。

## 局限性

- **硬件要求**：运行大型语言模型对电脑的硬件，特别是 GPU 性能要求较高。一些复杂模型在普通配置的电脑上运行速度可能较慢，甚至无法运行。
- **模型规模受限**：相比云端大规模的计算资源，本地硬件资源毕竟有限，因此在运行超大型模型时，可能会受到内存、显存等限制，难以发挥模型的全部潜力。
- **功能更新滞后**：与云服务提供商持续更新和优化的模型服务相比，Ollama 依赖社区和开源贡献，在功能更新和新特性支持上可能会存在一定的滞后性。

## 常见模型

- **Llama 系列**：包括 Llama 3.2 的 1B、3B 版本，Llama 3.2 Vision 的 11B、90B 版本，Llama 3.1 的 8B、405B 版本，还有 Llama 2 Uncensored 的 7B 版本等。
- **DeepSeek-R1**：有 7B、14B、671B 等版本，该模型由中国的深度求索公司开发，能用较低成本和算力实现较好效果。
- **Phi 系列**：如 Phi 4 的 14B 版本和 Phi 4 Mini 的 3.8B 版本。
- **Gemma 2**：包含 2B、9B、27B 等版本。
- **Qwen系列**：有7B、14B、32B、72B（部分为量化版）等版本，开发者为阿里云，广泛应用于中文场景。
- **其他**：还有 Mistral 7B、Moondream 2 1.4B、Neural Chat 7B、Starling 7B、Code Llama 7B、LLaVA 7B、Granite-3.2 8B 等版本。

不同版本的大模型在性能和功能上存在诸多区别，主要体现在参数量、运算速度、任务处理能力、功能特性等方面，以下是具体介绍：



- **参数量与知识容量**1：通常参数量越多，模型能捕捉的语言规律越复杂，知识容量越大。如 DeepSeek-R1 的 1.5B 版本参数量较少，适合处理基础任务，而 671B 版本作为基础大模型，具备专家级推理能力，可用于气候建模、基因组分析等前沿任务。
- **运算速度与响应时间**1：小参数量模型如 Gemma 3-1B，运行速度快，能快速给出响应，适合对实时性要求高的场景，如移动端的简单问答。而大参数量模型如 DeepSeek-R1-70B，虽然处理复杂任务能力强，但运算耗时较长，对硬件要求高，响应时间相对较长。
- **任务处理准确性**1：小模型在简单任务上表现较好，如短文本生成、基础问答等，但处理复杂逻辑推理、长文本分析等任务时，准确性可能较低。中型模型如 DeepSeek-R1-8B 在保证一定速度的同时，提升了文本生成和逻辑推理的准确性，可用于代码生成、逻辑推理等任务。大型和超大型模型则在金融预测、复杂数据分析等对准确性要求高的场景中更具优势。
- **上下文理解能力**2：部分大模型版本具备更长的上下文窗口，如 Gemma 3 的 12B 和 27B 版本，拥有 128K 长上下文窗口，可处理约 300 页书籍或 1 小时视频内容，能更好地理解长篇文本的语境，生成更连贯、准确的回答，而小模型的上下文处理能力相对较弱。
- **多模态能力**2：一些模型的特定版本支持多模态功能，如 Gemma 3 的 12B 和 27B 版本，可支持图像、短视频和文本输入，能构建文档解析、视觉问答等智能分析应用，而其小参数量版本可能不具备此功能。
- **语言支持能力**2：不同版本在语言支持上有差异，部分模型版本可开箱即用支持 35 种以上语言，预训练覆盖 140 多种语言，在非英语任务中也能有较好表现，适合全球化应用开发，而一些轻量级版本可能只支持常见的几种语言。
- **安全与特殊功能**2：某些高阶版本可能会集成额外的安全防护手段，如 Gemma 3 附带 ShieldGemma 2 图像安全分类器，可检测暴力、色情等内容，保障模型输出的安全性，而低版本或轻量级版本可能没有这类安全机制。

## 常用操作（Win上）

### 安装 Ollama

1. **下载安装包**：前往 Ollama 的[官方 GitHub 发布页面](https://github.com/jmorganca/ollama/releases) ，找到适用于 Windows 系统的安装包（通常是`.exe`文件），点击下载。
2. **运行安装程序**：下载完成后，双击安装包，按照安装向导的提示进行操作，比如选择安装路径等，完成 Ollama 的安装。

### 启动 Ollama 服务

安装完成后，在开始菜单中找到 Ollama 应用并打开，它会在后台启动 Ollama 服务。

或者：

```bash
ollama server
```

看到相关启动提示信息即表示服务已成功启动。

### 模型管理

#### 列出可用模型

```bash
ollama list
```

就能查看已经下载到本地的模型列表，列表中会显示模型的名称、大小等信息。

#### 拉取模型

```bash
ollama pull [model_name]
```

例如，想要拉取 Llama 2 模型，输入`ollama pull llama2`并回车，Ollama 会自动从模型仓库获取并下载该模型到本地。

#### 删除模型

```
ollama delete [model_name]
```

比如删除 Mistral 模型，输入`ollama delete mistral` 。

### 与模型交互

#### 命令行交互（启动指定模型）

```bash
ollama run [model_name]
```

即可与指定模型进行对话。

#### 调整模型参数

在运行模型时，可以通过添加参数来调整模型的行为。

```bash
ollama run --temperature 0.6 --max-tokens 300 llama2
```

这表示以温度 0.6、最大生成 300 个 token 的设置来运行 Llama 2 模型 。其中，`--temperature`参数控制生成文本的随机性，值越高随机性越强；`--max-tokens`参数用于设置生成文本的最大长度。

## 查看ollama的模型存储位置

终端运行：

```bash
echo $OLLAMA_MODELS
```

如果返回一个路径（如 `/data/ollama/models`），则 Ollama 模型存储在该位置。

## 更改Ollama下载模型的位置

可通过设置环境变量或修改服务配置文件实现。

Windows系统：

1. 右键点击“此电脑”，选择“属性”，进入“高级系统设置”。
2. 点击“环境变量”，在“系统变量”或“用户变量”中点击“新建”。
3. 变量名输入`OLLAMA_MODELS`，变量值输入目标路径（如`D:\Ollama\Models`）。
4. 保存后重启Ollama服务使更改生效。

Linux系统：

1. 关闭Ollama服务：`sudo systemctl stop ollama`。
2. 创建新模型存储目录，如`/data/ollama/models`，并设置权限：`sudo chown -R root:root /data/ollama/models`，`sudo chmod -R 775 /data/ollama/models`。
3. 编辑Ollama服务配置文件`/etc/systemd/system/ollama.service`，在`[Service]`部分添加`Environment="OLLAMA_MODELS=/data/ollama/models"`。
4. 重载配置并重启服务：`sudo systemctl daemon-reload`，`sudo systemctl restart ollama`。

# LlamaFactory

## 安装

0.环境准备，创建虚拟环境

```bash
conda create -n llama-factory python=3.10

conda activate llama-factory
```

1.克隆仓库

```  bash
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git  
cd LLaMA-Factory 
```

**`--depth 1`**：仅克隆最近一次提交的版本（浅克隆），大幅减少下载数据量，适合快速获取最新代码（但无法查看历史提交记录）。

2.安装依赖

pip install -e ".[torch,metrics]"  # 基础依赖（含PyTorch）  

- 若需GPU加速：需手动安装对应CUDA版本的PyTorch（避免默认安装CPU版本）：

  ```bash
  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu122  # 以CUDA 12.2为例  
  ```

- 可选依赖

  （如需量化、分布式训练等）：

  ~~~bash
  pip install bitsandbytes  # 量化支持  
  pip install deepspeed  # 分布式训练  
  ```[7,9](@ref)  
  ~~~

3.验证安装

```bash
llamafactory-cli version  
```

## 启动WebUI

```bash
llamafactory-cli webui  
```

访问`http://localhost:7860`即可看到可视化界面



# compass

在Ubuntu上部署OpenCompass（大模型评测工具），可按以下**核心步骤**操作（以2025年最新版本为例）：

### **1. 准备环境**

先安装Python和Conda（推荐使用Conda管理虚拟环境，避免依赖冲突）：

```
# 安装Miniconda（若已安装可跳过）  
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh  
bash Miniconda3-latest-Linux-x86_64.sh  
# 激活Conda  
source ~/.bashrc  
```

### **2. 创建虚拟环境**

新建一个名为`opencompass`的Python 3.10环境：

```
conda create --name opencompass python=3.10 -y  
conda activate opencompass  # 激活环境  
```

### **3. 安装OpenCompass**

推荐通过**源码安装**（支持自定义配置）：

```
# 克隆项目仓库  
git clone https://github.com/open-compass/opencompass.git  
cd opencompass  
# 安装依赖（自动处理）  
pip install -e .  
```

*注：若需快速安装，也可用`pip install opencompass`，但可能缺少部分功能。*

### **4. 配置评测资源**

#### **① 选择数据集**

- **内置数据集**：直接使用（如`MMLU`、`HumanEval`），系统会自动下载。
- **自定义数据集**：需手动下载并放置到`opencompass/data/`目录。

#### **② 选择模型**

- 本地模型：需下载模型文件（如.bin格式），在configs/models/下新建配置文件，指定模型路径。

  例：

  ```
  # 示例：配置本地LLaMA模型  
  models = [dict(type='LocalModel', path='/path/to/llama-7b.bin')]  
  ```

- API模型：需获取API密钥和服务地址（如OpenAI、InternLM），在configs/models/下配置API参数。

  例：

  ```
  # 示例：配置InternLM API  
  models = [dict(type='OpenAISDK',  
                 api_key='your-api-key',  
                 api_base='https://api.internlm.com/v1',  
                 model_name='internlm-7b')]  
  ```

### **5. 运行评测**

通过命令行指定数据集和模型，启动评测：

```
# 评测本地模型（示例：用MMLU数据集测试LLaMA）  
python run.py --datasets MMLU --models llama-7b --debug  

# 评测API模型（示例：用HumanEval数据集测试GPT-4）  
python run.py --datasets HumanEval --models gpt-4 --api  
```

*参数说明*：

- `--datasets`：指定数据集名称（支持多个，用逗号分隔）。
- `--models`：指定模型名称（需与配置文件一致）。
- `--debug`：输出详细日志（可选）。

### **6. 查看结果**

评测结果会自动生成在`results/`目录，包含：

- **表格报告**：显示各项任务的得分（如准确率、生成质量）。
- **可视化图表**：支持雷达图、趋势图，直观对比模型表现。

### **注意事项**

- **本地模型**：需确保Ubuntu服务器有GPU（推荐NVIDIA RTX 3090/4090），否则无法运行大模型。
- **API模型**：需保证网络畅通，部分API可能有调用次数限制（如OpenAI免费版每月2000次）。
- **依赖冲突**：若安装时出现冲突，可尝试新建干净的Conda环境重新安装。

以上步骤参考自OpenCompass官方文档及社区实践，适合快速部署和基础评测。如需高级功能（如分布式评测、自定义指标），可参考官方文档进一步配置。

数据集准备：

在 OpenCompass 项目根目录下运行下面命令，将数据集准备至 `${OpenCompass}/data` 目录下：

```bash
wget https://github.com/open-compass/opencompass/releases/download/0.1.8.rc1/OpenCompassData-core-20231110.zip
unzip OpenCompassData-core-20231110.zip
```

如果需要使用 OpenCompass 提供的更加完整的数据集 (~500M)，可以使用下述命令进行下载：

```bash
wget https://github.com/open-compass/opencompass/releases/download/0.1.8.rc1/OpenCompassData-complete-20231110.zip
unzip OpenCompassData-complete-20231110.zip  （解压完后发现就会有data目录了）
cd ./data
unzip *.zip
```

OpenCompass 已经支持了大多数常用于性能比较的数据集，具体支持的数据集列表请直接在`configs/datasets`下进行查找。



## 三. 准备评测

我评测的是llama2和qwen的模型，我就介绍一下我做的流程。

### 1. 准备评测配置文件

拷贝并修改如下文件`${OPEN_COMPASS_HOME}/configs/eval_qwen_7b_base.py`

```python3
from mmengine.config import read_base

with read_base():
    from .models.qwen.hf_qwen_7b import models
    # from .datasets.collections.leaderboard.qwen import datasets    # 使用所有数据集
    # from .summarizers.leaderboard import summarizer    # 使用summarizer做全数据集展示
    # from .datasets.ceval.ceval_ppl_578f8d import ceval_datasets    # 使用ceval数据集
    from .datasets.ceval.ceval_gen_5f30c7 import ceval_datasets    # 使用ceval数据集
    from .datasets.cmmlu.cmmlu_gen_c13365 import cmmlu_datasets    # 使用cmmlu数据集
    # from .datasets.cmmlu.cmmlu_ppl_8b9c76 import cmmlu_datasets    # 使用cmmlu数据集
    


# datasets = [*piqa_datasets, *siqa_datasets]
# datasets = [*ceval_datasets]
datasets = [*cmmlu_datasets, *ceval_datasets]
```

本段配置代码主要输出两个重要配置：

- 评测数据集`datasets`，当前是ceval和cmmlu，也可以选择多个评测集
- 待评测模型`models`，当前是qwen_7b_base，其中`.models.qwen.hf_qwen_7b_base` 对应的是文件`${OPEN_COMPASS_HOME}/configs/models/qwen/hf_qwen_7b_base.py`

### 2. 修改模型配置

OpenCompass默认直接使用HuggingFace model ID从hf上下载模型和权重，如果本地已有模型权重，可以通过如下方式来指定：
修改模型配置文件`${OPEN_COMPASS_HOME}/configs/models/qwen/hf_qwen_7b_base.py`，即刚刚在上个配置文件中指定的模型配置:

```text
from opencompass.models import HuggingFaceCausalLM

models = [
    dict(
        type=HuggingFaceCausalLM,
        abbr='qwen-7b—sft-hf',    # 运行完结果展示的名称
        path="/data/user/qwen_sft_checkpoint",    # 模型路径
        tokenizer_path='/data/user/qwen_sft_checkpoint',    # 分词器路径
        model_kwargs=dict(
            device_map='auto',
            trust_remote_code=True,
        ),
        tokenizer_kwargs=dict(
            padding_side='left',
            truncation_side='left',
            trust_remote_code=True,
            use_fast=False,
        ),
        pad_token_id=151643,
        max_out_len=100,
        max_seq_len=2048,
        batch_size=16,
        run_cfg=dict(num_gpus=8, num_procs=8),    # 我使用A800 开启8张卡评测，就算使用1，也默认使用多卡
    )
]
```

将`path` 和 `tokenizer_path`指向本地模型目录即可。

## 四. 启动评测

直接使用项目根目录下的run.py，指定我们定义好的评测配置，即可运行：

```text
python run.py configs/eval_qwen_7b_base.py
```

### 留意并查看出错日志

运行过程中留意stdout中给出的WARNING和ERROR，如果worker出错，程序不会中断，但已经没有必要往下跑，出错日志在日志文件中，stdout中会有提示路径，自己提早打开错误日志文件查看：
类似：

```text
10/07 15:43:30 - OpenCompass - WARNING - task OpenICLInfer[qwen-7b-base-hf/ceval-college_economics,qwen-7b-base-hf/ceval-accountant,qwen-7b-base-hf/ceval-tax_accountant,qwen-7b-base-hf/ceval-physician,qwen-7b-base-hf/ceval-civil_servant ... 2-7b-base-hf/ceval-discrete_mathematics,qwen-7b-base-hf/ceval-middle_school_geography] fail, see
./outputs/default/20231007_154225/logs/infer/qwen-7b-base-hf/ceval-college_economics.out
```

## 五. 评测结果

评测完成后，会打印评测结果表格如下：

```text
dataset                                         version    metric    mode     qwen-7b-base-hf
----------------------------------------------  ---------  --------  ------  ---------------
cmmlu-agronomy                                  18e759     accuracy  ppl                 59.17
cmmlu-anatomy                                   e2b12f     accuracy  ppl                 42.57
cmmlu-ancient_chinese                           7faca5     accuracy  ppl                 32.32
cmmlu-arts                                      cc4476     accuracy  ppl                 90.62
cmmlu-astronomy                                 b81d60     accuracy  ppl                 42.42
cmmlu-business_ethics                           5a294c     accuracy  ppl                 63.64
cmmlu-chinese_civil_service_exam                d2f770     accuracy  ppl                 59.38
cmmlu-chinese_driving_rule                      0e8a93     accuracy  ppl                 77.86
cmmlu-chinese_food_culture                      8e2055     accuracy  ppl                 61.03
cmmlu-chinese_foreign_policy                    484713     accuracy  ppl                 76.64
cmmlu-chinese_history                           b336e0     accuracy  ppl                 89.16
cmmlu-chinese_literature                        a9a252     accuracy  ppl                 53.92
cmmlu-chinese_teacher_qualification             eacc26     accuracy  ppl                 78.21
cmmlu-clinical_knowledge                        9993e7     accuracy  ppl                 54.85
cmmlu-college_actuarial_science                 1f3eb3     accuracy  ppl                 26.42
cmmlu-college_education                         7dbe6b     accuracy  ppl                 74.77
cmmlu-college_engineering_hydrology             44f211     accuracy  ppl                 49.06
cmmlu-college_law                               524735     accuracy  ppl                 58.33
cmmlu-college_mathematics                       b94953     accuracy  ppl                 28.57
cmmlu-college_medical_statistics                96aa86     accuracy  ppl                 51.89
cmmlu-college_medicine                          5d2d59     accuracy  ppl                 62.27
cmmlu-computer_science                          ca1243     accuracy  ppl                 58.82
cmmlu-computer_security                         c21394     accuracy  ppl                 73.1
cmmlu-conceptual_physics                        6dffab     accuracy  ppl                 79.59
cmmlu-construction_project_management           4dd438     accuracy  ppl                 47.48
cmmlu-economics                                 2a5014     accuracy  ppl                 62.89
cmmlu-education                                 8fb812     accuracy  ppl                 66.26
cmmlu-electrical_engineering                    c692a4     accuracy  ppl                 52.91
cmmlu-elementary_chinese                        ecace0     accuracy  ppl                 63.49
cmmlu-elementary_commonsense                    f0a3cc     accuracy  ppl                 67.17
cmmlu-elementary_information_and_technology     edaf57     accuracy  ppl                 78.15
cmmlu-elementary_mathematics                    099f4c     accuracy  ppl                 43.48
cmmlu-ethnology                                 201b78     accuracy  ppl                 56.3
cmmlu-food_science                              ff494e     accuracy  ppl                 65.73
cmmlu-genetics                                  aa522b     accuracy  ppl                 57.39
cmmlu-global_facts                              0e84cf     accuracy  ppl                 69.8
cmmlu-high_school_biology                       4114ed     accuracy  ppl                 81.07
cmmlu-high_school_chemistry                     540bcf     accuracy  ppl                 72.73
cmmlu-high_school_geography                     70e849     accuracy  ppl                 68.64
cmmlu-high_school_mathematics                   04285b     accuracy  ppl                 35.98
cmmlu-high_school_physics                       d09967     accuracy  ppl                 60.91
cmmlu-high_school_politics                      d047b7     accuracy  ppl                 73.43
cmmlu-human_sexuality                           cc75c8     accuracy  ppl                 60.32
cmmlu-international_law                         f420cb     accuracy  ppl                 50.27
cmmlu-journalism                                5ef649     accuracy  ppl                 63.95
cmmlu-jurisprudence                             6a38fc     accuracy  ppl                 66.18
cmmlu-legal_and_moral_basis                     6c3e30     accuracy  ppl                 92.99
cmmlu-logical                                   56a1ca     accuracy  ppl                 50.41
cmmlu-machine_learning                          f26e28     accuracy  ppl                 51.64
cmmlu-management                                ed634c     accuracy  ppl                 74.29
cmmlu-marketing                                 acb874     accuracy  ppl                 66.67
cmmlu-marxist_theory                            1fd3ba     accuracy  ppl                 82.01
cmmlu-modern_chinese                            805e5f     accuracy  ppl                 53.45
cmmlu-nutrition                                 49ad23     accuracy  ppl                 62.76
cmmlu-philosophy                                71f30b     accuracy  ppl                 69.52
cmmlu-professional_accounting                   f1c72e     accuracy  ppl                 69.71
cmmlu-professional_law                          2651ae     accuracy  ppl                 48.82
cmmlu-professional_medicine                     0a6df4     accuracy  ppl                 44.95
cmmlu-professional_psychology                   d689f4     accuracy  ppl                 76.72
cmmlu-public_relations                          1091db     accuracy  ppl                 62.07
cmmlu-security_study                            69d6ff     accuracy  ppl                 71.11
cmmlu-sociology                                 d58f6c     accuracy  ppl                 58.85
cmmlu-sports_science                            e669b1     accuracy  ppl                 63.64
cmmlu-traditional_chinese_medicine              60cd30     accuracy  ppl                 61.62
cmmlu-virology                                  4ace32     accuracy  ppl                 65.68
cmmlu-world_history                             b012c4     accuracy  ppl                 87.58
cmmlu-world_religions                           c59782     accuracy  ppl                 73.12
ceval-computer_network                          9b9417     accuracy  ppl                 52.63
ceval-operating_system                          b2b8cf     accuracy  ppl                 42.11
ceval-computer_architecture                     1bd275     accuracy  ppl                 66.67
ceval-college_programming                       2d0833     accuracy  ppl                 54.05
ceval-college_physics                           fb7e04     accuracy  ppl                 21.05
ceval-college_chemistry                         916b7d     accuracy  ppl                 58.33
ceval-advanced_mathematics                      5cad2a     accuracy  ppl                 21.05
ceval-probability_and_statistics                a6b30e     accuracy  ppl                 27.78
ceval-discrete_mathematics                      68be68     accuracy  ppl                 25
ceval-electrical_engineer                       056c2e     accuracy  ppl                 35.14
ceval-metrology_engineer                        4a757a     accuracy  ppl                 70.83
ceval-high_school_mathematics                   a8ed21     accuracy  ppl                 16.67
ceval-high_school_physics                       e1fc86     accuracy  ppl                 73.68
ceval-high_school_chemistry                     9021c6     accuracy  ppl                 68.42
ceval-high_school_biology                       c7f5a1     accuracy  ppl                 73.68
ceval-middle_school_mathematics                 213989     accuracy  ppl                 47.37
ceval-middle_school_biology                     ce0420     accuracy  ppl                 95.24
ceval-middle_school_physics                     78f3af     accuracy  ppl                 84.21
ceval-middle_school_chemistry                   d071d2     accuracy  ppl                 95
ceval-veterinary_medicine                       cd3a07     accuracy  ppl                 52.17
ceval-college_economics                         a35346     accuracy  ppl                 49.09
ceval-business_administration                   69dd6a     accuracy  ppl                 60.61
ceval-marxism                                   283ce0     accuracy  ppl                 78.95
ceval-mao_zedong_thought                        f38cd1     accuracy  ppl                 70.83
ceval-education_science                         fbd65c     accuracy  ppl                 75.86
ceval-teacher_qualification                     c77f1f     accuracy  ppl                 88.64
ceval-high_school_politics                      bbac37     accuracy  ppl                 100
ceval-high_school_geography                     730a30     accuracy  ppl                 84.21
ceval-middle_school_politics                    15b2d7     accuracy  ppl                 90.48
ceval-middle_school_geography                   b00167     accuracy  ppl                 91.67
ceval-modern_chinese_history                    5a04cd     accuracy  ppl                 82.61
ceval-ideological_and_moral_cultivation         0829ff     accuracy  ppl                 84.21
ceval-logic                                     c9c394     accuracy  ppl                 59.09
ceval-law                                       cbd3c5     accuracy  ppl                 29.17
ceval-chinese_language_and_literature           716ab3     accuracy  ppl                 43.48
ceval-art_studies                               476114     accuracy  ppl                 60.61
ceval-professional_tour_guide                   70f30f     accuracy  ppl                 72.41
ceval-legal_professional                        f19cf5     accuracy  ppl                 52.17
ceval-high_school_chinese                       931614     accuracy  ppl                 84.21
ceval-high_school_history                       4d6364     accuracy  ppl                 90
ceval-middle_school_history                     7f6356     accuracy  ppl                100
ceval-civil_servant                             a5dcb8     accuracy  ppl                 61.7
ceval-sports_science                            192553     accuracy  ppl                 52.63
ceval-plant_protection                          f7ff86     accuracy  ppl                 68.18
ceval-basic_medicine                            a95a09     accuracy  ppl                 68.42
ceval-clinical_medicine                         664b54     accuracy  ppl                 59.09
ceval-urban_and_rural_planner                   fdae6f     accuracy  ppl                 63.04
ceval-accountant                                d810a1     accuracy  ppl                 59.18
ceval-fire_engineer                             bb924d     accuracy  ppl                 58.06
ceval-environmental_impact_assessment_engineer  d59200     accuracy  ppl                 61.29
ceval-tax_accountant                            9e16f2     accuracy  ppl                 44.9
ceval-physician                                 0e90d5     accuracy  ppl                 63.27
```

所有过程的日志，预测，以及最终结果会放在`opencompass/outputs/default/`目录下。目录结构如下所示：

![img](https://pic4.zhimg.com/v2-5dc277e6421dd46745631d7af057501b_1440w.jpg)

```text
outputs/default/
├── 20231127_160345
├── ...
├── 20231129_112843   # 每次评测，以日期前缀命名
│   ├── configs       # 每次实验都会在此处存下用于追溯的 config
│   ├── logs          # 运行日志
│   │   ├── eval
│   │   └── infer
│   ├── predictions   # 储存了每个任务的推理结果
│   ├── results       # 每个数据集不同类别的评测结果
│   └── summary       # 评测结果
│   │   ├── summary_20231129_112843.csv
│   │   └── summary_20231129_112843.txt
├── ...
```

## 六. 分值计算（根据自己需求）

我把上面的展示结果复制到txt文件中，写脚本进行平均值计算，分别算ppl值和gen值，ppl值一般用来评测base模型，gen值一般用来评测sft模型。

代码入下：

```python3
import re

def process_file(file_path):
    # 用于存储 ceval 和 cmmlu 行的数字
    ceval_numbers = []
    cmmlu_numbers = []

    # 读取文件
    with open(file_path, 'r') as file:
        # 处理每一行
        for line in file:
            match = re.search(r'(\d+\.\d+|\d+)$', line)
            if match:
                # 如果找到数字，将其添加到相应的列表中
                number = float(match.group(1))

                if line.startswith('ceval'):
                    ceval_numbers.append(number)
                elif line.startswith('cmmlu'):
                    cmmlu_numbers.append(number)

    return ceval_numbers, cmmlu_numbers

file_path = '/data/user/model/qwen_7b_base.txt'

ceval_numbers, cmmlu_numbers = process_file(file_path)

print(f"ceval 平均值: {sum(ceval_numbers) / len(ceval_numbers)}")

print(f"cmmlu 平均值: {sum(cmmlu_numbers) / len(cmmlu_numbers)}")


# 结果是：
#   ceval 平均值: 63.25269230769231
#   cmmlu 平均值: 62.63805970149252
```
