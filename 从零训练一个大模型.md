# 基础知识

从零开始训练一个大模型（如GPT、BERT等）是一项复杂的系统工程，涉及硬件、数据、算法、工程化等多个环节。以下是关键步骤和注意事项的详细总结，适合有一定技术背景的读者参考：

### **一、明确目标与资源规划**

#### 1. 确定模型类型与规模

- **任务类型**：明确模型用途（文本生成、知识问答、多模态等），不同任务对应不同的架构（如Transformer、CNN+RNN等）。
- **规模选择**：根据资源决定参数量级（如1B、7B、175B）。小规模模型（<1B）可在单机多卡训练；超大规模（>10B）需分布式集群。

#### 2. 硬件资源评估

- 算力需求：训练大模型主要依赖GPU/TPU。例如：

  - 1B参数模型：需数十块高端GPU（如A100/H100）。
  - 7B+模型：需数百块GPU，甚至依赖超级计算机或云计算平台（如AWS、GCP）。
  
- **存储与带宽**：模型参数、中间激活值、数据集需高速存储（NVMe SSD）和网络（InfiniBand）支持。

- **成本估算**：训练一次175B模型（如GPT-3）成本可能高达数百万美元（电费+硬件折旧）。

### **二、数据准备：质量与规模决定上限**

#### 1. 数据来源与规模

- **文本数据**：通用领域可用Common Crawl、Wikipedia、BooksCorpus；垂直领域需行业数据（如医疗、法律）。
- **数据量级**：1B参数模型通常需要TB级文本（如1-10TB清洗后数据）；更大模型需PB级。

#### 2. 数据清洗与预处理

- **去噪**：过滤垃圾文本（广告、乱码）、重复内容、低质量网页。

- **去重**：基于MinHash/LSH算法去除相似文档，避免模型过拟合。

- **敏感信息处理**：删除个人隐私、版权内容（如GDPR合规）。

- 分词与编码：

  - 使用Byte-Pair Encoding (BPE) 或WordPiece生成子词词汇表（如30k-100k tokens）。
- 对文本进行分词并转换为Token ID序列。

#### 3. 数据增强与平衡

- **多语言支持**：若需多语言模型，需平衡不同语言的数据比例（如英语60%、中文20%等）。
- **领域平衡**：避免单一领域数据主导（如通用文本与专业文本比例调整）。

### **三、模型架构设计**

#### 1. 基础架构选择

- **Transformer架构**：主流选择（如GPT的Decoder-only、BERT的Encoder-only、T5的Encoder-Decoder）。

- 关键超参数：

  - 层数（Layer）：如12层（BERT-base）、24层（GPT-3）。
- 隐藏层维度（Hidden Size）：如768（BERT-base）、1600（GPT-3 175B）。
  - 注意力头数（Attention Heads）：如12（BERT-base）、96（GPT-3 175B）。
- 上下文窗口（Context Length）：如512（早期模型）、32k（PaLM）。

#### 2. 优化与改进

- **稀疏注意力**：如Longformer的局部+全局注意力，降低计算复杂度。
- **混合精度训练**：使用FP16/BF16减少显存占用，加速计算（需支持Tensor Core的GPU）。
- **激活检查点**：用时间换显存，减少中间激活值的存储开销。

### **四、分布式训练策略**

#### 1. 数据并行（Data Parallelism）

- 将数据分片到多个设备，每个设备持有完整模型副本，同步梯度更新（适合单机多卡或小规模集群）。

#### 2. 模型并行（Model Parallelism）

- **张量并行**：将模型的单个层拆分到不同设备（如矩阵乘法分块）。
- **流水线并行**：将模型按层分组，不同设备负责不同层的计算（需平衡计算负载）。

#### 3. 高级并行技术

- **Megatron-LM**：结合张量并行+流水线并行，支持万亿参数训练。
- **DeepSpeed**：提供ZeRO（零冗余优化器）技术，减少显存占用，支持千亿级模型。
- **FSDP（Fully Sharded Data Parallel）**：PyTorch原生支持，进一步优化显存分配。

#### 4. 通信优化

- 使用NCCL库加速GPU间通信，减少同步延迟。
- 梯度压缩（如1-bit Adam）降低通信带宽需求。

### **五、训练过程管理**

#### 1. 优化器与学习率调度

- **优化器**：AdamW（带权重衰减）、LAMB（适合大Batch训练）。
- **学习率**：采用Warmup+Decay策略（如前1k步线性增长，后余弦衰减）。
- **Batch Size**：从数千到数百万不等（需配合梯度累积模拟大Batch）。

#### 2. 正则化与稳定性

- **Dropout**：在注意力层和FFN层添加（如0.1-0.3概率）。
- **权重衰减**：防止过拟合（如0.01）。
- **梯度裁剪**：限制梯度范数（如1.0），避免爆炸。

#### 3. 监控与调试

- **日志工具**：使用Weights & Biases (W&B)、TensorBoard跟踪损失、学习率、梯度分布。
- **Checkpointing**：定期保存模型状态（如每1k步），支持断点续训。
- **错误排查**：常见坑点包括显存溢出（OOM）、梯度NaN、数据加载瓶颈。

### **六、评估与调优**

#### 1. 评估指标

- **困惑度（Perplexity）**：衡量语言模型对测试数据的预测不确定性（越低越好）。
- **下游任务微调**：在特定任务（如文本分类、问答）上微调后评估准确率/F1值。

#### 2. 消融实验

- 测试不同超参数（如层数、学习率）对性能的影响，找到最优配置。

### **七、部署与推理优化**

#### 1. 模型压缩

- **量化**：将FP32权重转为INT8/FP16，减少推理显存占用（可能损失少量精度）。
- **剪枝**：移除冗余神经元或注意力头，加速计算。

#### 2. 推理加速

- **KV Cache**：缓存注意力键值对，避免重复计算（适用于自回归生成任务）。
- **动态批处理**：合并多个请求的输入，提高GPU利用率。

### **八、伦理与合规**

- **偏见检测**：评估模型输出是否包含性别、种族等偏见（如使用StereoSet数据集）。
- **内容过滤**：部署时加入安全规则，防止生成有害内容（如暴力、虚假信息）。

### **九、持续迭代**

- **数据更新**：定期加入新数据（如新闻、社交媒体），保持模型时效性。
- **模型微调**：针对用户反馈进行领域适配（如医疗、法律垂直领域）。

### **关键挑战与建议**

- **算力门槛**：中小团队可考虑开源预训练模型（如LLaMA、GPT-NeoX）进行微调，而非从零训练。
- **数据质量**：80%的效果提升可能来自高质量数据清洗和标注。
- **工程化能力**：分布式训练、容错机制、监控系统是大规模训练的核心。

------

从零训练大模型需要跨学科协作（算法、工程、运维），建议优先评估资源可行性，或基于现有开源框架（如Megatron-LM、DeepSpeed）快速启动。若资源有限，可聚焦于特定场景的微调或小规模模型开发。



# OLMo 1B 预训练

## 1.准备工作

### 1.1 创建虚拟环境



### 1.2 下载源码



### 1.3 安装配套依赖









## 训练流程



## 训练数据集



## wandb使用







## 重要模型参数





 ## 训练参数

 



## 分词器选择与使用



## 断点续训

./out文件夹：

.pth文件：保存**模型权重参数**

.pth文件：保存优化器状态





## 训练过程分析





## 测评







# 指令微调



instruction:

input:

output:

history:



## 对SFT数据集处理

### 数据清洗



## 0.前期准备

安装cuda：

https://developer.nvidia.com/

验证：

Windows：

```bash
nvcc -V
```

Linux：

```bash
nvidia-smi
```

安装conda：

https://docs.anaconda.com/miniconda/

## 1.安装llama factory

https://github.com/hiyouga/LLaMA-Factory
https://llamafactory.readthedocs.io/zh-cn/latest/

1.克隆仓库

```  bash
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git  
```

**`--depth 1`**：仅克隆最近一次提交的版本（浅克隆），大幅减少下载数据量，适合快速获取最新代码（但无法查看历史提交记录）。

2.创建虚拟环境：

```bash
conda create -n llama-factory python=3.10
conda activate llama-factory
```

3.安装依赖

```bash
cd LLaMA-Factory
pip install -e ".[torch,metrics]"  # 基础依赖（含PyTorch）  
```

4.校验

```bash
import torch
torch.cuda.current device()
torch.cuda.get_device_name(0)
torch. __version__
```

torch.cuda.current device()：获取当前所选 CUDA 设备索引，返回当前正在使用的 CUDA 设备的索引（从 0 开始）。如果没有可用的 CUDA 设备，或者 CUDA 不可用，调用这个函数可能会引发错误或返回无效值。

`torch.cuda.get_device_name(0)` 用于获取指定 CUDA 设备（这里是设备索引 `0`）的名称（即 GPU 型号）。返回指定 CUDA 设备的名称（字符串形式），例如 `"NVIDIA GeForce RTX 3090"` 或 `"Tesla V100"`。如果指定的设备索引无效（如超出范围或 CUDA 不可用），可能会引发错误。

`torch.__version__` 是一个内置属性，用于获取当前安装的 PyTorch 版本号。返回当前 PyTorch 的版本字符串（如 `"2.1.0"`、`"1.13.1"` 等）。可用于检查 PyTorch 版本，确保兼容性或调试版本相关的问题。如果没有后缀，说明不支持GPU，需要重新下载。



## 2.模型下载

创建文件夹存放原始模型：

```bash
mkdir llm
```

魔搭社区中可以找到所有开源模型：

```bash
git clone https://www.modelscope.cn/models/Qwen/Qwen2.5-0.5B-Instruct.git
```

## 3.模型推理

先把模型跑起来：

```bash
llamafactory-cli webchat --model_name_or_path D:/llm-sft/Qwen2.5-0.5B-Instruct --template qwen
```

执行命令后，会跳转到一个网址(localhost:7860)：



## 4.SFT数据集下载（电商数据集）

魔搭社区：

```bash
git clone https://www.modelscope.cn/datasets/whisperF/AdvertiseGen.git
```

将数据放在LLaMA-Factory/data文件夹

LLaMA-Factory/data/AdvertiseGen：

dev.json

train.json



## 5.数据集注册

为了让训练时可以找到要用的json文件

在data/dataset_info.json文件中添加：

```json
    "adgen_local":{
​        "file name":"AdvertiseGen/train.json",
​        "columns" :{
​		 "prompt":"content",
​		 "response" :"summary"
​        }
​   }
```

"file name"：文件位置。

"columns" ：转义。将"content"转换成"prompt"，"summary"转换成"response"。



## 6.基于Lora的SFT指令微调

从：

```bash
lamafactory-cli train
  --stage sft \
  --do_train \
  --model_name_or_path D:/LLaMA-Factory/llm/Qwen2.5-0.5B-Instruct \
  --dataset adgen_local \
  --dataset_dir ./data \
  --template qwen \
  --finetuning_type lora \
  --output_dir ./saves/Qwen2/lora/sft \
  --overwrite cache \
  --overwrite output dir \
  --cutoff_len 1024 \
  --preprocessing_num_workers 16 \
  --per_device_train_batch_size 2 \
  --per_device_eval_batch_size 1 \
  --gradient_accumulation_steps 8 \
  --lr_scheduler_type cosine \
  ...
```

model_name_or_path：原始模型的路径

dataset：在data/dataset_info.json文件添加的数据集名称

dataset_dir：数据集的路径

template：

finetuning_type：

output_dir：模型训练后输出的路径

overwrite_cache：

overwrite_oyput_dir：

：

：

：

：





## 7.合并Lora推理

```bash
llamafactory-cli webchat \
  --model_name_or_path D:/LLaMA-Factory/llm/Qwen2.5-0.5B-Instruct \
  --adapter_name_or_path ./saves/Qwen2/lora/sft \
  --template qwen \
  --finetuning_type lora
```



```bash
llamafactory-cli webchat --model_name_or_path D:/LLaMA-Factory/llm/Qwen2.5-0.5B-nstruct --adapter_name_or_path ./saves/Qwen2/lora/sft --template qwen --finetuning_type lora
```



## 8. 训练效果评估

```bash
pip install jieba
pip install rouge-chinese
pip install nltk
```

jieba：

rouge-chinese：

nltk：NLTK（**Natural Language Toolkit**，自然语言工具包）是 Python 中最流行的自然语言处理（NLP）库之一，提供了丰富的工具和数据集，用于文本处理、分词、词性标注、句法分析、情感分析等任务。

```bash
lamafactory-cli train
  --stage sft \
  --do_predict \
  --model_name_or_path D:/LLaMA-Factory/llm/Qwen2.5-0.5B-Instruct \
  --adapter_name_or_path ./saves/Qwen2/lora/sft \
  --eval_dataset adgen_local \
  --dataset_dir ./data \
  --template qwen \
  --finetuning_type lora \
  --output_dir ./saves/Qwen2/lora/predict \
  --overwrite_cache \
  --overwrite_output_dir \
  --cutoff_len 1024 \
  --preprocessing_num_workers 1 \
  --per_device_eval_batch-size 1 \
  --max_samples 20 \
  --predict_with_generate
```



## 9.LoRA模型合并导出

```bash
llamafactory-cli export \
  --model_name_or_path D:/LLaMA-Factory/llm/Qwen2.5-0.5B-Instruct \
  --adapter_name_or_path ./saves/Qwen2/lora/sft \
  --template qwen \
  --finetuning_type lora \
  --export_dir_megred-model-path \ 
  --export_size 2 \
  --export_device cpu \
  --export_legacy_format False
```



## 10.一站式webui board的使用

```bash
llamafactory-cli webui
```



## 11.主流大模型评测 benchmark

```bash
llamafactory-cli eval \
  --model_name_or_path D:/code/LLaMA-Factory/llm/Qwen2.5-0.5B-Instruct \
  --template qwen \
  --task mmlu_test \
  --lang en \
  --n_shot 5 \
  --batch_size 1
  --trust_remote_code True
```

mmlu：MMLU（Massive Multitask Language Understanding，大规模多任务语言理解）是用于评估语言模型跨学科知识与推理能力的基准测试。
